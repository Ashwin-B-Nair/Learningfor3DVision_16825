{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/ashwi/Documents/CMU/Spring_25/Learning_3D_Vision/assignment1/assignment1-master/abijunai_code_proj1/s tarter')\n",
    "\n",
    "import dolly_zoom \n",
    "import utils \n",
    "import pytorch3d\n",
    "from pytorch3d.utils import ico_sphere\n",
    "import pytorch3d.renderer\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import camera_transforms\n",
    "import render_generic\n",
    "import mcubes\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 1.1.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 1. Practicing with Cameras\n",
    "#-----------------------------------------------------------------\n",
    "# 1.1. 360-degree Renders (5 points)\n",
    "\n",
    "vertices, faces = utils.load_cow_mesh(path=\"data/cow.obj\")\n",
    "vertices = vertices.unsqueeze(0)\n",
    "faces = faces.unsqueeze(0)\n",
    "textures = torch.ones_like(vertices)  # (1, N_v, 3)\n",
    "textures = textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(textures),\n",
    ").to(device)\n",
    "\n",
    "number_views = 50\n",
    "azimuth = np.linspace(-180, 180, num=number_views)  \n",
    "\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist = 3, elev = 30, azim =azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "\n",
    "image_size = 256\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "my_images = renderer(mesh.extend(number_views), cameras= cameras, lights= lights)\n",
    "my_images = my_images.cpu().numpy()[..., :3]\n",
    "my_images = (my_images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/360_cow.gif\", my_images, fps=12, loop = 0)\n",
    "\n",
    "print(\"End of 1.1.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:45<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 1.2\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Re-creating the Dolly Zoom (10 points)\n",
    "dolly_zoom.dolly_zoom(num_frames=30, output_file=\"results/dolly_zoom.gif\")\n",
    "\n",
    "print(\"End of 1.2\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 2.1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#2. Practicing with Meshes\n",
    "#-----------------------------------------------------------------\n",
    "##2.1 Constructing a Tetrahedron (5 points)\n",
    "\n",
    "t_vertices = torch.tensor([\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [0.5, 0.866, 0],\n",
    "        [0.5, 0.289, 0.816]\n",
    "    ])\n",
    "\n",
    "t_faces = torch.tensor([\n",
    "    [0, 1, 2],  \n",
    "    [0, 2, 3],  \n",
    "    [0, 3, 1],  \n",
    "    [1, 3, 2]  \n",
    "])\n",
    "\n",
    "t_vertices = t_vertices.unsqueeze(0)\n",
    "t_faces = t_faces.unsqueeze(0)\n",
    "t_textures = torch.ones_like(t_vertices)  # (1, N_v, 3)\n",
    "t_textures = t_textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=t_vertices,\n",
    "    faces=t_faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(t_textures),\n",
    ").to(device)\n",
    "\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist = 3, elev = 15, azim =azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "image_size = 256\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "my_images = renderer(mesh.extend(number_views), cameras= cameras, lights= lights)\n",
    "my_images = my_images.cpu().numpy()[..., :3]\n",
    "my_images = (my_images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/360_tetrahedron.gif\", my_images, fps=12, loop = 0)\n",
    "\n",
    "print(\"End of 2.1.\")\n",
    "\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 2.2.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "##2.2. Constructing a Cube (5 points)\n",
    "\n",
    "c_vertices = torch.tensor([\n",
    "        [0, 0, 0],\n",
    "        [1, 0, 0],\n",
    "        [1, 1, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 1],\n",
    "        [1, 1, 1],\n",
    "        [0, 1, 1]\n",
    "    ])\n",
    "c_vertices = c_vertices.float()\n",
    "\n",
    "c_faces = torch.tensor([\n",
    "        [0, 1, 2],\n",
    "        [0, 2, 3],\n",
    "        [4, 6, 5],\n",
    "        [4, 7, 6],\n",
    "        [0, 5, 1],\n",
    "        [0, 4, 5],\n",
    "        [1, 6, 2],\n",
    "        [1, 5, 6],\n",
    "        [2, 7, 3],\n",
    "        [2, 6, 7],\n",
    "        [3, 4, 0],\n",
    "        [3, 7, 4]\n",
    "    ])\n",
    "\n",
    "c_vertices = c_vertices.unsqueeze(0)\n",
    "c_faces = c_faces.unsqueeze(0)\n",
    "t_textures = torch.ones_like(c_vertices)  # (1, N_v, 3)\n",
    "t_textures = t_textures * torch.tensor([0.7, 0.7, 1])  # (1, N_v, 3)\n",
    "mesh = pytorch3d.structures.Meshes(\n",
    "    verts=c_vertices,\n",
    "    faces=c_faces,\n",
    "    textures=pytorch3d.renderer.TexturesVertex(t_textures),\n",
    ").to(device)\n",
    "\n",
    "number_views = 50\n",
    "image_size = 256\n",
    "azimuth = np.linspace(-180, 180, num=number_views)  \n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist = 5, elev = 30, azim =azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "my_images = renderer(mesh.extend(number_views), cameras= cameras, lights= lights)\n",
    "my_images = my_images.cpu().numpy()[..., :3]\n",
    "my_images = (my_images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/360_cube.gif\", my_images, fps=12, loop = 0)\n",
    "\n",
    "print(\"End of 2.2.\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 3.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 3. Re-texturing a mesh (10 points)\n",
    "vertices, faces = utils.load_cow_mesh(path=\"data/cow.obj\")\n",
    "\n",
    "textures = torch.ones_like(vertices)\n",
    "\n",
    "z_min = torch.min(vertices[:, 2])\n",
    "z_max = torch.max(vertices[:, 2])\n",
    "color1 = torch.tensor([0, 0, 1])\n",
    "color2 = torch.tensor([1, 0, 0])\n",
    "\n",
    "for i in range(len(vertices)):\n",
    "    alpha = (vertices[i,2] - z_min) / (z_max - z_min)\n",
    "    color = alpha * (color2) + (1 - alpha) * (color1)\n",
    "    textures[i]= color\n",
    "\n",
    "vertices = vertices.unsqueeze(0)\n",
    "faces = faces.unsqueeze(0)    \n",
    "textures = textures.unsqueeze(0)\n",
    "retexture = pytorch3d.renderer.TexturesVertex(textures.to(device))\n",
    "\n",
    "re_meshes = pytorch3d.structures.Meshes(\n",
    "    verts=vertices,\n",
    "    faces=faces,\n",
    "    textures=retexture,\n",
    ").to(device)\n",
    "\n",
    "number_views = 50\n",
    "azimuth = np.linspace(-180, 180, num=number_views)\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist = 3, elev = 30, azim =azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "image_size = 256\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "\n",
    "images = renderer(re_meshes.extend(number_views), cameras= cameras, lights= lights)\n",
    "images = images.cpu().numpy()[..., :3]\n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/restexture.gif\", images, fps=12, loop = 0)\n",
    "\n",
    "print(\"End of 3.\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/ashwi/Documents/CMU/Spring_25/Learning_3D_Vision/assignment1/assignment1-master/abijunai_code_proj1/starter\\camera_transforms.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  R_relative = torch.tensor(R_relative).float()\n",
      "C:\\Users/ashwi/Documents/CMU/Spring_25/Learning_3D_Vision/assignment1/assignment1-master/abijunai_code_proj1/starter\\camera_transforms.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  T_relative = torch.tensor(T_relative).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 4.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 4. Camera Transformations (10 points)\n",
    "\n",
    "# Image 1\n",
    "R_relative_1 = torch.tensor([\n",
    "        [0, 1, 0],\n",
    "        [-1, 0, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "T_relative_1 = torch.tensor([0, 0, 0]) \n",
    "\n",
    "image1 = camera_transforms.render_textured_cow(R_relative = R_relative_1, T_relative=T_relative_1)\n",
    "plt.imsave(\"results/cow_trans_1_Q4.jpg\", image1)\n",
    "\n",
    "# Image 2\n",
    "R_relative_2 = torch.tensor([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "T_relative_2 = torch.tensor([0, 0, 3]) \n",
    "\n",
    "image2 = camera_transforms.render_textured_cow(R_relative = R_relative_2, T_relative=T_relative_2)\n",
    "plt.imsave(\"results/cow_trans_2_Q4.jpg\", image2)\n",
    "\n",
    "# Image 3\n",
    "R_relative_3 = torch.tensor([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "T_relative_3 = torch.tensor([0.5, -0.5, 0]) \n",
    "\n",
    "image3 = camera_transforms.render_textured_cow(R_relative = R_relative_3, T_relative=T_relative_3)\n",
    "plt.imsave(\"results/cow_trans_3_Q4.jpg\", image3)\n",
    "\n",
    "# Image 4\n",
    "R_relative_4 = torch.tensor([\n",
    "        [0, 0, 1 ],\n",
    "        [0, 1, 0],\n",
    "        [-1, 0, 0]\n",
    "    ])\n",
    "T_relative_4 = torch.tensor([-3, 0, 3]) \n",
    "image4 = camera_transforms.render_textured_cow(R_relative = R_relative_4, T_relative=T_relative_4)\n",
    "plt.imsave(\"results/cow_trans_4_Q4.jpg\", image4)\n",
    "\n",
    "print(\"End of 4.\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 5. Rendering Generic 3D Representations\n",
    "\n",
    "# 5.1 Rendering Point Clouds from RGB-D Images (10 points)\n",
    "data = render_generic.load_rgbd_data()\n",
    "\n",
    "points_1, colors_1 = utils.unproject_depth_image(torch.Tensor(data[\"rgb1\"]), \n",
    "                                              torch.Tensor(data[\"mask1\"]),\n",
    "                                              torch.Tensor(data[\"depth1\"]),\n",
    "                                              data[\"cameras1\"])\n",
    "\n",
    "point_cloud_1 = pytorch3d.structures.Pointclouds(points=points_1.unsqueeze(0), features=colors_1.unsqueeze(0)).to(device)\n",
    "number_views = 10\n",
    "azimuth = np.linspace(-180, 180, num=number_views)\n",
    "image_size = 256\n",
    "\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist = 10, elev = 0, azim =azimuth)\n",
    "\n",
    "#flip point cloud\n",
    "R = torch.tensor([\n",
    "        [-1, 0, 0],\n",
    "        [0, -1, 0],\n",
    "        [0, 0, 1]\n",
    "    ]).float() @ R\n",
    "\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0, -3]], device=device)\n",
    "renderer = utils.get_points_renderer(image_size=image_size, device=device)\n",
    "\n",
    "images = renderer(point_cloud_1.extend(number_views), cameras= cameras, lights= lights)\n",
    "images = images.cpu().numpy()[..., :3] \n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/point_cloud_1.gif\", images, fps=4, loop = 0)\n",
    "print(\"End of 5.1- point cloud 1\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_2, colors_2 = utils.unproject_depth_image(torch.Tensor(data[\"rgb2\"]), \n",
    "                                              torch.Tensor(data[\"mask2\"]),\n",
    "                                              torch.Tensor(data[\"depth2\"]),\n",
    "                                              data[\"cameras2\"])\n",
    "point_cloud_2 = pytorch3d.structures.Pointclouds(points=points_2.unsqueeze(0), features=colors_2.unsqueeze(0))\n",
    "\n",
    "images = renderer(point_cloud_2.extend(number_views), cameras= cameras, lights= lights)\n",
    "images = images.cpu().numpy()[..., :3] \n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/point_cloud_2.gif\", images, fps=4, loop = 0)\n",
    "print(\"End of 5.1- point cloud 2\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "point_cloud_3 = pc3 = pytorch3d.structures.Pointclouds(points=torch.cat((points_1,points_2), 0).unsqueeze(0),\n",
    "    features=torch.cat((colors_1,colors_2), 0).unsqueeze(0),).to(device)\n",
    "\n",
    "images = renderer(point_cloud_3.extend(number_views), cameras= cameras, lights= lights)\n",
    "images = images.cpu().numpy()[..., :3] \n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/point_cloud_3.gif\", images, fps=5, loop = 0)\n",
    "print(\"End of 5.1- point cloud 3\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 5.2\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 5.2 Parametric Functions (10 + 5 points)\n",
    "\n",
    "phi = torch.linspace(0, 2 * np.pi, 100)\n",
    "theta = torch.linspace(0, 2 * np.pi, 100)\n",
    "phi, theta = torch.meshgrid(phi, theta)\n",
    "\n",
    "x = (3 + torch.cos(theta)) * torch.cos(phi)\n",
    "y = (3 + torch.cos(theta)) * torch.sin(phi)\n",
    "z = torch.sin(theta)\n",
    "\n",
    "points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1).unsqueeze(0)\n",
    "color = (points - points.min()) / (points.max() - points.min()).unsqueeze(0)\n",
    "\n",
    "torus_point_cloud = pytorch3d.structures.Pointclouds(points=points, features=color)\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist=10, elev=0, azim=azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "renderer = utils.get_points_renderer(image_size=image_size, device=device)\n",
    "images = renderer(torus_point_cloud.extend(number_views), cameras=cameras)\n",
    "images = images.cpu().numpy()[..., :3]\n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/torus.gif\", images, fps=12, loop=0)\n",
    "\n",
    "print(\"End of 5.2- torus\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 5.2- new object\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for the catenoid\n",
    "a = 1.0  \n",
    "u = torch.linspace(0, 2 * np.pi, 100)  \n",
    "v = torch.linspace(-2, 2, 100)  \n",
    "u, v = torch.meshgrid(u, v)\n",
    "\n",
    "# Parametric equations for the catenoid\n",
    "x = a * torch.cosh(v/a) * torch.cos(u)\n",
    "y = a * torch.cosh(v/a) * torch.sin(u)\n",
    "z = v\n",
    "\n",
    "points = torch.stack((x.flatten(), y.flatten(), z.flatten()), dim=1).unsqueeze(0)\n",
    "color = (points - points.min()) / (points.max() - points.min())\n",
    "\n",
    "catenoid_point_cloud = pytorch3d.structures.Pointclouds(points=points, features=color)\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist=10, elev=0, azim=azimuth)\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "renderer = utils.get_points_renderer(image_size=image_size, device=device)\n",
    "images = renderer(catenoid_point_cloud.extend(number_views), cameras=cameras)\n",
    "images = images.cpu().numpy()[..., :3]\n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/catenoid.gif\", images, fps=12, loop=0)\n",
    "\n",
    "print(\"End of 5.2- new object\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 5.3\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 5.3 Implicit Surfaces (15 + 5 points)\n",
    "device = utils.get_device()\n",
    "image_size = 256 \n",
    "min_value = -3.1\n",
    "max_value = 3.1\n",
    "voxel_size = 64\n",
    "X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "\n",
    "voxels = (X ** 2 + Y ** 2 + Z ** 2 + 2 ** 2 - 1 ** 2) ** 2 - 4 * 2 ** 2 * (X ** 2 + Y ** 2)\n",
    "vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "vertices = torch.tensor(vertices).float()\n",
    "faces = torch.tensor(faces.astype(int))\n",
    "# Vertex coordinates are indexed by array position, so we need to\n",
    "# renormalize the coordinate system.\n",
    "vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(device)\n",
    "\n",
    "number_views = 25\n",
    "azimuth = np.linspace(-180, 180, num=number_views)\n",
    "\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device,)\n",
    "\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist=15, elev=0, azim=azimuth)\n",
    "\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "\n",
    "images = renderer(mesh.extend(number_views), cameras=cameras, lights = lights)\n",
    "images = images.cpu().numpy()[..., :3]\n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/torus_implicit.gif\", images, fps=12, loop=0)\n",
    "print(\"End of 5.3\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 5.3- new object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = utils.get_device()\n",
    "image_size = 256 \n",
    "min_value = -3.1\n",
    "max_value = 3.1\n",
    "voxel_size = 64\n",
    "\n",
    "X, Y, Z = torch.meshgrid([torch.linspace(min_value, max_value, voxel_size)] * 3)\n",
    "\n",
    "voxels = torch.cos(X)*torch.sin(Y) + torch.cos(Y)*torch.sin(Z) + torch.cos(Z)*torch.sin(X)\n",
    "\n",
    "vertices, faces = mcubes.marching_cubes(mcubes.smooth(voxels), isovalue=0)\n",
    "vertices = torch.tensor(vertices).float()\n",
    "faces = torch.tensor(faces.astype(int))\n",
    "# Vertex coordinates are indexed by array position, so we need to\n",
    "# renormalize the coordinate system.\n",
    "vertices = (vertices / voxel_size) * (max_value - min_value) + min_value\n",
    "textures = (vertices - vertices.min()) / (vertices.max() - vertices.min())\n",
    "textures = pytorch3d.renderer.TexturesVertex(vertices.unsqueeze(0))\n",
    "\n",
    "mesh = pytorch3d.structures.Meshes([vertices], [faces], textures=textures).to(device)\n",
    "\n",
    "number_views = 50\n",
    "azimuth = np.linspace(-180, 180, num=number_views)\n",
    "\n",
    "lights = pytorch3d.renderer.PointLights(location=[[0, 0.0, -4.0]], device=device,)\n",
    "\n",
    "renderer = utils.get_mesh_renderer(image_size=image_size, device=device)\n",
    "\n",
    "R, T = pytorch3d.renderer.look_at_view_transform(dist=15, elev=0, azim=azimuth)\n",
    "\n",
    "cameras = pytorch3d.renderer.FoVPerspectiveCameras(R=R, T=T, device=device)\n",
    "\n",
    "images = renderer(mesh.extend(number_views), cameras=cameras)\n",
    "images = images.cpu().numpy()[..., :3]\n",
    "images = (images * 255).clip(0, 255).astype(np.uint8)\n",
    "imageio.mimsave(\"results/gyroid_implicit.gif\", images, fps=12, loop=0)\n",
    "\n",
    "print(\"End of 5.3- new object\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 6.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# 6. Do Something Fun (10 points)\n",
    "\n",
    "print(\"End of 6.\")\n",
    "#-----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 7.\n"
     ]
    }
   ],
   "source": [
    "#-----------------------------------------------------------------\n",
    "# (Extra Credit) 7. Sampling Points on Meshes (10 points\n",
    "def sample_mesh_to_point_cloud(mesh_path, num_samples):\n",
    "\n",
    "    vertices, faces, _ = pytorch3d.io.load_obj(mesh_path)\n",
    "    \n",
    "    #Finding the vertices of each face\n",
    "    face_vertices = vertices[faces.verts_idx]\n",
    "    vertices_0 = face_vertices[:, 0]\n",
    "    vertices_1 = face_vertices[:, 1]\n",
    "    vertices_2 = face_vertices[:, 2]\n",
    "    \n",
    "    return face_vertices\n",
    "\n",
    "\n",
    "mesh_path = \"data/cow.obj\"\n",
    "\n",
    "point_cloud = sample_mesh_to_point_cloud(mesh_path, num_samples=10)\n",
    "\n",
    "\n",
    "\n",
    "print(\"End of 7.\")\n",
    "#-----------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch3d_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
